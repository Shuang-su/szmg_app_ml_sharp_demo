#!/usr/bin/env python3
"""
ä¸‹è½½mirrorsç›®å½•ä¸­æ–‡ç« çš„å¤–é“¾å›¾ç‰‡åˆ°æœ¬åœ°
"""

import os
import re
import requests
import hashlib
from pathlib import Path
from urllib.parse import urlparse

MIRRORS_DIR = Path(__file__).parent.parent / 'mirrors'

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
    'Referer': 'https://www.sztv.com.cn/',
}

def get_image_filename(url):
    """ä»URLç”Ÿæˆæœ¬åœ°æ–‡ä»¶å"""
    # å°è¯•ä»URLè·å–åŸå§‹æ–‡ä»¶å
    parsed = urlparse(url)
    path = parsed.path
    filename = os.path.basename(path)
    
    # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œä½¿ç”¨URLçš„hash
    if not filename or len(filename) < 5:
        url_hash = hashlib.md5(url.encode()).hexdigest()[:16]
        ext = '.jpg'  # é»˜è®¤æ‰©å±•å
        if '.png' in url.lower():
            ext = '.png'
        elif '.gif' in url.lower():
            ext = '.gif'
        elif '.webp' in url.lower():
            ext = '.webp'
        filename = f"img_{url_hash}{ext}"
    
    return filename

def download_image(url, save_path):
    """ä¸‹è½½å›¾ç‰‡"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return True
    except Exception as e:
        print(f"    ä¸‹è½½å¤±è´¥: {e}")
    return False

def process_html_file(html_path):
    """å¤„ç†å•ä¸ªHTMLæ–‡ä»¶ï¼Œä¸‹è½½å¤–é“¾å›¾ç‰‡å¹¶æ›´æ–°å¼•ç”¨"""
    print(f"\nğŸ“„ å¤„ç†: {html_path}")
    
    with open(html_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾æ‰€æœ‰å¤–é“¾å›¾ç‰‡URL - æ›´å®½æ¾çš„åŒ¹é…
    # åŒ¹é…æ‰€æœ‰ https://...png/jpg/jpeg/gif/webp
    img_pattern = r'(https?://[^\s"\'<>]+\.(?:png|jpg|jpeg|gif|webp)(?:\?[^\s"\'<>]*)?)'
    matches = re.findall(img_pattern, content, re.IGNORECASE)
    
    if not matches:
        print("  æ²¡æœ‰æ‰¾åˆ°å¤–é“¾å›¾ç‰‡")
        return 0
    
    # å»é‡
    urls = list(set(matches))
    print(f"  æ‰¾åˆ° {len(urls)} ä¸ªå¤–é“¾å›¾ç‰‡")
    
    download_count = 0
    parent_dir = html_path.parent
    
    for url in urls:
        # æ¸…ç†URLï¼ˆç§»é™¤æŸ¥è¯¢å‚æ•°ç”¨äºæ–‡ä»¶åï¼‰
        clean_url = url.split('?')[0]
        filename = get_image_filename(clean_url)
        save_path = parent_dir / filename
        
        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        if save_path.exists() and save_path.stat().st_size > 100:
            print(f"  âœ“ å·²å­˜åœ¨: {filename}")
        else:
            print(f"  â¬‡ ä¸‹è½½: {filename}")
            if download_image(url, save_path):
                download_count += 1
                print(f"    âœ“ æˆåŠŸ ({save_path.stat().st_size} bytes)")
            else:
                print(f"    âœ— å¤±è´¥")
                continue
        
        # æ›´æ–°HTMLä¸­çš„å¼•ç”¨
        content = content.replace(url, filename)
    
    # ä¿å­˜æ›´æ–°åçš„HTML
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"  å·²æ›´æ–°HTMLå¼•ç”¨")
    return download_count

def main():
    print("=" * 50)
    print("å¼€å§‹ä¸‹è½½å¤–é“¾å›¾ç‰‡")
    print("=" * 50)
    
    total_downloaded = 0
    
    # éå†æ‰€æœ‰mirrorså­ç›®å½•
    for dir_path in sorted(MIRRORS_DIR.iterdir()):
        if dir_path.is_dir() and not dir_path.name.startswith('.'):
            html_file = dir_path / 'index.html'
            if html_file.exists():
                count = process_html_file(html_file)
                total_downloaded += count
    
    print("\n" + "=" * 50)
    print(f"å®Œæˆï¼å…±ä¸‹è½½ {total_downloaded} å¼ å›¾ç‰‡")
    print("=" * 50)

if __name__ == '__main__':
    main()
