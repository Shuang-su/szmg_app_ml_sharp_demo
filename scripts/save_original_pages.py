#!/usr/bin/env python3
"""
ä¿å­˜åŸå§‹ç½‘é¡µè„šæœ¬
ä¸‹è½½æ–‡ç« é¡µé¢çš„HTMLã€CSSã€JSç­‰èµ„æº
"""

import requests
import os
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from pathlib import Path

# æ–‡ç« åˆ—è¡¨
ARTICLES = [
    ("80611955", "https://www.sztv.com.cn/ysz/zx/zbsz/80611955.shtml"),
    ("80611627", "https://www.sztv.com.cn/ysz/zx/rd/80611627.shtml"),
    ("80611833", "https://www.sztv.com.cn/ysz/zx/tj/80611833.shtml"),
    ("80611814", "https://www.sztv.com.cn/ysz/zx/tj/80611814.shtml"),
    ("80611789", "https://www.sztv.com.cn/ysz/zx/tj/80611789.shtml"),
    ("80611791", "https://www.sztv.com.cn/ysz/zx/tj/80611791.shtml"),
    ("80611586", "https://www.sztv.com.cn/ysz/zx/zw/80611586.shtml"),
    ("80611296", "https://www.sztv.com.cn/ysz/zx/zw/80611296.shtml"),
    ("80611248", "https://www.sztv.com.cn/ysz/zx/zw/80611248.shtml"),
    ("80611001", "https://www.sztv.com.cn/ysz/zx/tj/80611001.shtml"),
    ("80611004", "https://www.sztv.com.cn/ysz/zx/tj/80611004.shtml"),
    ("80611058", "https://www.sztv.com.cn/ysz/zx/tj/80611058.shtml"),
]

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}

def ensure_dir(path):
    Path(path).mkdir(parents=True, exist_ok=True)

def save_resource(url, save_path, base_url):
    """ä¸‹è½½å¹¶ä¿å­˜èµ„æºæ–‡ä»¶"""
    try:
        full_url = urljoin(base_url, url)
        response = requests.get(full_url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return True
    except Exception as e:
        print(f"  ä¸‹è½½å¤±è´¥ {url}: {e}")
    return False

def save_article(article_id, url, output_dir):
    """ä¿å­˜å•ç¯‡æ–‡ç« åŠå…¶èµ„æº"""
    print(f"\nğŸ“„ ä¿å­˜æ–‡ç« : {article_id}")
    
    article_dir = Path(output_dir) / article_id
    ensure_dir(article_dir)
    ensure_dir(article_dir / 'css')
    ensure_dir(article_dir / 'js')
    ensure_dir(article_dir / 'img')
    
    try:
        # è·å–HTML
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.encoding = 'utf-8'
        html_content = response.text
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ä¿å­˜åŸå§‹HTML
        with open(article_dir / 'original.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"  âœ“ HTMLå·²ä¿å­˜")
        
        # æå–å¹¶ä¿å­˜CSS
        css_count = 0
        for i, link in enumerate(soup.find_all('link', rel='stylesheet')):
            href = link.get('href')
            if href:
                css_filename = f"style_{i}.css"
                if save_resource(href, article_dir / 'css' / css_filename, url):
                    css_count += 1
                    # æ›´æ–°HTMLä¸­çš„å¼•ç”¨
                    link['href'] = f"css/{css_filename}"
        print(f"  âœ“ CSS: {css_count} ä¸ªæ–‡ä»¶")
        
        # æå–å¹¶ä¿å­˜JS
        js_count = 0
        for i, script in enumerate(soup.find_all('script', src=True)):
            src = script.get('src')
            if src:
                js_filename = f"script_{i}.js"
                if save_resource(src, article_dir / 'js' / js_filename, url):
                    js_count += 1
                    script['src'] = f"js/{js_filename}"
        print(f"  âœ“ JS: {js_count} ä¸ªæ–‡ä»¶")
        
        # æå–å¹¶ä¿å­˜å›¾ç‰‡
        img_count = 0
        for i, img in enumerate(soup.find_all('img')):
            src = img.get('src') or img.get('data-src')
            if src:
                src = str(src)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                if src.startswith('data:'):
                    continue
                ext = os.path.splitext(urlparse(src).path)[1] or '.jpg'
                img_filename = f"img_{i}{ext}"
                if save_resource(src, article_dir / 'img' / img_filename, url):
                    img_count += 1
                    if img.get('src'):
                        img['src'] = f"img/{img_filename}"
                    if img.get('data-src'):
                        img['data-src'] = f"img/{img_filename}"
        print(f"  âœ“ å›¾ç‰‡: {img_count} ä¸ªæ–‡ä»¶")
        
        # ä¿å­˜å¤„ç†åçš„HTMLï¼ˆæœ¬åœ°èµ„æºç‰ˆæœ¬ï¼‰
        with open(article_dir / 'index.html', 'w', encoding='utf-8') as f:
            f.write(str(soup))
        print(f"  âœ“ æœ¬åœ°åŒ–HTMLå·²ä¿å­˜")
        
        return True
        
    except Exception as e:
        print(f"  âœ— ä¿å­˜å¤±è´¥: {e}")
        return False

def main():
    base_dir = Path(__file__).parent.parent
    output_dir = base_dir / 'data' / 'original_pages'
    ensure_dir(output_dir)
    
    print("=" * 50)
    print("å¼€å§‹ä¿å­˜åŸå§‹ç½‘é¡µ")
    print("=" * 50)
    
    success_count = 0
    for article_id, url in ARTICLES:
        if save_article(article_id, url, output_dir):
            success_count += 1
    
    print("\n" + "=" * 50)
    print(f"å®Œæˆï¼æˆåŠŸä¿å­˜ {success_count}/{len(ARTICLES)} ç¯‡æ–‡ç« ")
    print(f"ä¿å­˜ä½ç½®: {output_dir}")
    print("=" * 50)

if __name__ == '__main__':
    main()
